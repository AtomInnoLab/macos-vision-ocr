{
  "texts" : "The Llama 3.2-Vision Collection of multimodal large langyage model5 （LLMS） is a\ncollection of instruction-tuned image reasoning generative models in l1B and 90B\nsizes （text + images in \/ text ovt）. The Llama 3.2-Vision instruction-tuned models\nare optimized for visval recognittion, iage reasoning, captioning, and answering\ngeneral qvestions about an iage. The models outperform many of the available\nopen Source and Closed multimodal models on common industry benchmarKs.",
  "info" : {
    "height" : 720,
    "filename" : "handwriting.webp",
    "filepath" : "\/Users\/abao\/Documents\/Github\/macos-vision-ocr\/.\/images\/handwriting.webp",
    "width" : 1600
  },
  "observations" : [
    {
      "confidence" : 0.5,
      "text" : "The Llama 3.2-Vision Collection of multimodal large langyage model5 （LLMS） is a",
      "quad" : {
        "bottomLeft" : {
          "y" : 0.35483871098527953,
          "x" : 0.09011629800287288
        },
        "topRight" : {
          "x" : 0.87936045388666206,
          "y" : 0.28333333395755611
        },
        "topLeft" : {
          "y" : 0.28333333395755611,
          "x" : 0.09011629800287288
        },
        "bottomRight" : {
          "x" : 0.87936045388666206,
          "y" : 0.35483871098527953
        }
      }
    },
    {
      "text" : "collection of instruction-tuned image reasoning generative models in l1B and 90B",
      "confidence" : 0.5,
      "quad" : {
        "bottomLeft" : {
          "x" : 0.090116268965468371,
          "y" : 0.43225806298465586
        },
        "bottomRight" : {
          "y" : 0.43225806298465586,
          "x" : 0.88372092899964794
        },
        "topLeft" : {
          "x" : 0.090116268965468371,
          "y" : 0.36774193719967596
        },
        "topRight" : {
          "x" : 0.88372092899964794,
          "y" : 0.36774193719967596
        }
      }
    },
    {
      "quad" : {
        "bottomRight" : {
          "x" : 0.90988372519304139,
          "y" : 0.51290322458603832
        },
        "topRight" : {
          "y" : 0.44838709880105843,
          "x" : 0.90988372519304139
        },
        "topLeft" : {
          "y" : 0.44838709880105843,
          "x" : 0.090116269138353855
        },
        "bottomLeft" : {
          "x" : 0.090116269138353855,
          "y" : 0.51290322458603832
        }
      },
      "text" : "sizes （text + images in \/ text ovt）. The Llama 3.2-Vision instruction-tuned models",
      "confidence" : 0.5
    },
    {
      "confidence" : 0.5,
      "text" : "are optimized for visval recognittion, iage reasoning, captioning, and answering",
      "quad" : {
        "topLeft" : {
          "x" : 0.090116283028365843,
          "y" : 0.52580645030504303
        },
        "topRight" : {
          "x" : 0.87790696296000648,
          "y" : 0.52580645030504303
        },
        "bottomLeft" : {
          "x" : 0.090116283028365843,
          "y" : 0.59722222168345418
        },
        "bottomRight" : {
          "y" : 0.59722222168345418,
          "x" : 0.87790696296000648
        }
      }
    },
    {
      "confidence" : 0.5,
      "text" : "general qvestions about an iage. The models outperform many of the available",
      "quad" : {
        "topLeft" : {
          "x" : 0.090116291199582074,
          "y" : 0.61250000048202458
        },
        "bottomRight" : {
          "x" : 0.89534883514489449,
          "y" : 0.67777777825980234
        },
        "topRight" : {
          "y" : 0.61250000048202458,
          "x" : 0.89534883514489449
        },
        "bottomLeft" : {
          "y" : 0.67777777825980234,
          "x" : 0.090116291199582074
        }
      }
    },
    {
      "confidence" : 0.5,
      "quad" : {
        "bottomLeft" : {
          "y" : 0.75161290360794786,
          "x" : 0.090116302457303524
        },
        "topLeft" : {
          "y" : 0.69677419324689094,
          "x" : 0.090116302457303524
        },
        "topRight" : {
          "x" : 0.82558139034792843,
          "y" : 0.69677419324689094
        },
        "bottomRight" : {
          "y" : 0.75161290360794786,
          "x" : 0.82558139034792843
        }
      },
      "text" : "open Source and Closed multimodal models on common industry benchmarKs."
    }
  ]
}